{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import en samen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape written: (45000, 784)\n",
      "shape spoken (45000,)\n",
      "shape spoken indiv: (38, 13)\n",
      "13\n",
      "(45000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "written_train =numpy.load(\"written_train(1).npy\")\n",
    "spoken_train =numpy.load(\"spoken_train(1).npy\")\n",
    "match_train = numpy.load(\"match_train(1).npy\")\n",
    "feature_amount = spoken_train[0].shape[1]\n",
    "print(\"shape written:\", written_train.shape)\n",
    "print(\"shape spoken\", spoken_train.shape)\n",
    "print(\"shape spoken indiv:\", spoken_train[4].shape)\n",
    "print(feature_amount)\n",
    "print(match_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 93, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., -1,  0,  0],\n",
       "       [ 0,  0,  0, ..., -2,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ..., -1,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "maxs = []\n",
    "for i in range(0, len(spoken_train)):\n",
    "    maxs.append(spoken_train[i].shape[0])\n",
    "maxlen_spoken_train = max(maxs)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "spoken_train_3d= pad_sequences(spoken_train, maxlen= max(maxs))\n",
    "print(spoken_train_3d.shape)\n",
    "\n",
    "spoken_train_2d =spoken_train_3d.reshape(45000, maxlen_spoken_train*feature_amount)\n",
    "\n",
    "spoken_train_2d \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#Standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS_scaler = StandardScaler()\n",
    "spoken_train_SSscaler = SS_scaler.fit_transform(spoken_train_2d)\n",
    "written_train_SSscaler = SS_scaler.fit_transform(written_train)\n",
    "\n",
    "Scaler = \"Standard scaler\"\n",
    "\n",
    "#When using standard scaler\n",
    "X_values = numpy.hstack([spoken_train_SSscaler, written_train_SSscaler])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for standard scaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_values, X_acctest, match_train, y_acctest = train_test_split(X_values, match_train, test_size=1/3, random_state=999)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values, match_train, test_size=0.2, random_state=123)\n",
    "c,d = X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is first splitted into 'data for the model' and 'data to check'(acctest). The data for the model is then again splitted in train and test data. Afterwords when you get a good score you can test it on the check data (acctest) as if it were the data we had to submit in order to see how well it actually performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 21614\n",
      "Before OverSampling, counts of label '2': 2386 \n",
      "After OverSampling, the shape of X: (43228, 1993)\n",
      "After OverSampling, the shape of y: (43228,) \n",
      "\n",
      "After OverSampling, counts of label '1': 21614\n",
      "After OverSampling, counts of label '2': 21614\n",
      "(43228,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==0)))\n",
    "print(\"Before OverSampling, counts of label '2': {} \".format(sum(y_train==1)))\n",
    "\n",
    "sm = SMOTE(random_state=4)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of X: {}'.format(X_res.shape))\n",
    "print('After OverSampling, the shape of y: {} \\n'.format(y_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==0)))\n",
    "print(\"After OverSampling, counts of label '2': {}\".format(sum(y_res==1)))\n",
    "\n",
    "\n",
    "X= numpy.array(X_res)\n",
    "y= numpy.array(y_res)\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "print(y_train.shape)\n",
    "\n",
    "a, b = X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(a,b,1)\n",
    "X_test = X_test.reshape(c,d,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Janne/tutorial-env/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=2, activation=\"relu\", input_shape=(None, 199...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPool1D\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv1D(64, kernel_size=2, activation='relu', input_dim=b))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D())\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = y_train.reshape(a,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_54_input to have 3 dimensions, but got array with shape (43228, 1993)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-a9d03ceb2d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     metrics=['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_54_input to have 3 dimensions, but got array with shape (43228, 1993)"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict_classes(X_testecht, verbose=1) #moet X_test zijn!\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(1 in y_pred)\n",
    "y_pred = y_pred.reshape(15000,)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, = y_train.shape\n",
    "y_train = y_train.reshape((k,1))\n",
    "f, = y_test.shape\n",
    "y_test = y_test.reshape((f, 1))\n",
    "\n",
    "X_train = X_train.reshape(a,b,1)\n",
    "X_test = X_test.reshape(c,d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43228,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=1993))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43228/43228 [==============================] - 316s 7ms/step - loss: 0.5519 - acc: 0.7183\n",
      "Epoch 2/10\n",
      "43228/43228 [==============================] - 317s 7ms/step - loss: 0.4082 - acc: 0.8228\n",
      "Epoch 3/10\n",
      "43228/43228 [==============================] - 302s 7ms/step - loss: 0.3487 - acc: 0.8612\n",
      "Epoch 4/10\n",
      "43228/43228 [==============================] - 302s 7ms/step - loss: 0.3246 - acc: 0.8780\n",
      "Epoch 5/10\n",
      "43228/43228 [==============================] - 302s 7ms/step - loss: 0.3088 - acc: 0.8853\n",
      "Epoch 6/10\n",
      "43228/43228 [==============================] - 296s 7ms/step - loss: 0.2978 - acc: 0.8926\n",
      "Epoch 7/10\n",
      "43228/43228 [==============================] - 296s 7ms/step - loss: 0.2851 - acc: 0.8993\n",
      "Epoch 8/10\n",
      "43228/43228 [==============================] - 297s 7ms/step - loss: 0.2730 - acc: 0.9049\n",
      "Epoch 9/10\n",
      "43228/43228 [==============================] - 296s 7ms/step - loss: 0.2712 - acc: 0.9068\n",
      "Epoch 10/10\n",
      "43228/43228 [==============================] - 296s 7ms/step - loss: 0.2642 - acc: 0.9105\n",
      "6000/6000 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(1993, 1)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "written_test = numpy.load(\"written_test(1).npy\")\n",
    "spoken_test =  numpy.load(\"spoken_test(1).npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 93, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., -1,  0,  1],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ..., -2,  0,  1],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., -2, -1,  0],\n",
       "       [ 0,  0,  0, ..., -2, -1,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "maxs = []\n",
    "for i in range(0, len(spoken_test)):\n",
    "    maxs.append(spoken_test[i].shape[0])\n",
    "maxlen_spoken_test = max(maxs)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "spoken_test_3d= pad_sequences(spoken_test, maxlen= max(maxs))\n",
    "print(spoken_test_3d.shape)\n",
    "\n",
    "spoken_test_2d =spoken_test_3d.reshape(15000, maxlen_spoken_test*feature_amount)\n",
    "\n",
    "spoken_test_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS_scaler = StandardScaler()\n",
    "spoken_test_SSscaler = SS_scaler.fit_transform(spoken_test_2d)\n",
    "written_test_SSscaler = SS_scaler.fit_transform(written_test)\n",
    "\n",
    "Scaler = \"Standard scaler\"\n",
    "\n",
    "#When using standard scaler\n",
    "X_testecht = numpy.hstack([spoken_test_SSscaler, written_test_SSscaler])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acctest = X_acctest.reshape(15000,1993,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 29s 2ms/step\n",
      "[[0.11533731]\n",
      " [0.3047256 ]\n",
      " [0.1788943 ]\n",
      " ...\n",
      " [0.08398203]\n",
      " [0.13376065]\n",
      " [0.20282027]]\n",
      "False\n",
      "[0.11533731 0.3047256  0.1788943  ... 0.08398203 0.13376065 0.20282027]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_acctest, verbose=1) #moet X_test zijn!\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(1 in y_pred)\n",
    "y_pred = y_pred.reshape(15000,)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
