{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape written: (45000, 784)\n",
      "shape spoken (45000,)\n",
      "shape spoken indiv: (38, 13)\n",
      "13\n",
      "(45000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "written_train =numpy.load(\"written_train(1).npy\")\n",
    "spoken_train =numpy.load(\"spoken_train(1).npy\")\n",
    "match_train = numpy.load(\"match_train(1).npy\")\n",
    "feature_amount = spoken_train[0].shape[1]\n",
    "print(\"shape written:\", written_train.shape)\n",
    "print(\"shape spoken\", spoken_train.shape)\n",
    "print(\"shape spoken indiv:\", spoken_train[4].shape)\n",
    "print(feature_amount)\n",
    "print(match_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 93, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., -1,  0,  0],\n",
       "       [ 0,  0,  0, ..., -2,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ..., -1,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "maxs = []\n",
    "for i in range(0, len(spoken_train)):\n",
    "    maxs.append(spoken_train[i].shape[0])\n",
    "maxlen_spoken_train = max(maxs)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "spoken_train_3d= pad_sequences(spoken_train, maxlen= max(maxs))\n",
    "print(spoken_train_3d.shape)\n",
    "\n",
    "spoken_train_2d =spoken_train_3d.reshape(45000, maxlen_spoken_train*feature_amount)\n",
    "\n",
    "spoken_train_2d \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 784)\n",
      "(45000,)\n"
     ]
    }
   ],
   "source": [
    "#print(written_train[0])\n",
    "print(written_train.shape)\n",
    "print(match_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling/Standardizing/normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "spoken_train_mmsc = mm_scaler.fit_transform(spoken_train_2d)\n",
    "written_train_mmsc = mm_scaler.fit_transform(written_train)\n",
    "Scaler = \"MinMax\"\n",
    "\n",
    "#When using MinMax\n",
    "X_values = numpy.hstack([spoken_train_mmsc, written_train_mmsc])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for MinMax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#Standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS_scaler = StandardScaler()\n",
    "spoken_train_SSscaler = SS_scaler.fit_transform(spoken_train_2d)\n",
    "written_train_SSscaler = SS_scaler.fit_transform(written_train)\n",
    "\n",
    "Scaler = \"Standard scaler\"\n",
    "\n",
    "#When using standard scaler\n",
    "X_values = numpy.hstack([spoken_train_SSscaler, written_train_SSscaler])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for standard scaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#Max Abs\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "MaxAbs_scaler = MaxAbsScaler()\n",
    "spoken_train_MaxAbsSc = MaxAbs_scaler.fit_transform(spoken_train_2d)\n",
    "written_train_MaxAbsSc = MaxAbs_scaler.fit_transform(written_train)\n",
    "Scaler = \"Max Abs\"\n",
    "\n",
    "#When using Max Abs\n",
    "X_values = numpy.hstack([spoken_train_MaxAbsSc, written_train_MaxAbsSc])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for Max Abs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#Normalizing\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "spoken_train_normalized = normalize(spoken_train_2d, norm='l2')\n",
    "written_train_normalized = normalize(written_train, norm='l2')\n",
    "Scaler = \"Normalized\"\n",
    "\n",
    "#When using Normalized\n",
    "X_values = numpy.hstack([spoken_train_normalized, written_train_normalized])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for Normalizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#When using NOTHING!!!\n",
    "\n",
    "X_values = numpy.hstack([spoken_train_2d, written_train])\n",
    "print(X_values.shape)\n",
    "X_valSet = \"X_val set for NOTHING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample of oversample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority before upsampling: 4539\n",
      "Majority before upsampling: 40461\n",
      "na upsample: 40461\n"
     ]
    }
   ],
   "source": [
    "#------------ Separate majority and minority classes\n",
    "\n",
    "match_train_ = match_train.reshape((45000,1))\n",
    "df_combo = numpy.hstack([X_values, match_train_])\n",
    "\n",
    "#get row indices from True\n",
    "countT = 0\n",
    "rowindices_T = []\n",
    "for row in match_train:\n",
    "    if row == True:\n",
    "        rowindices_T.append(countT)\n",
    "    countT += 1\n",
    "df_minority = df_combo[rowindices_T][:]    \n",
    "df_minority.shape\n",
    "print(\"Minority before upsampling:\", df_minority.shape[0])\n",
    "\n",
    "\n",
    "#get row indices from False\n",
    "countF = 0\n",
    "rowindices_F = []\n",
    "for row in match_train:\n",
    "    if row == False:\n",
    "        rowindices_F.append(countF)\n",
    "    countF += 1\n",
    "df_majority = df_combo[rowindices_F][:]    \n",
    "\n",
    "aantalmajority = df_majority.shape[0]\n",
    "print(\"Majority before upsampling:\", aantalmajority)\n",
    "\n",
    "\n",
    "# ------------ Upsample minority class\n",
    "from sklearn.utils import resample\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples =aantalmajority,    # to match majority class\n",
    "\n",
    "                                 random_state=123) # reproducible results\n",
    "print(\"na upsample:\", df_minority_upsampled.shape[0])\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = numpy.vstack([df_majority, df_minority_upsampled])\n",
    "X_values_upsampled = df_upsampled[:,:-1]\n",
    "match_train_upsampled = df_upsampled[:,-1]\n",
    "X_train = X_values_upsampled\n",
    "y_train = match_train_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 40461\n",
      "Before OverSampling, counts of label '2': 4539 \n",
      "After OverSampling, the shape of X: (80922, 1993)\n",
      "After OverSampling, the shape of y: (80922,) \n",
      "\n",
      "After OverSampling, counts of label '1': 40461\n",
      "After OverSampling, counts of label '2': 40461\n",
      "(80922,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "y_train = match_train\n",
    "X_train = X_values\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==0)))\n",
    "print(\"Before OverSampling, counts of label '2': {} \".format(sum(y_train==1)))\n",
    "\n",
    "sm = SMOTE(random_state=4)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of X: {}'.format(X_res.shape))\n",
    "print('After OverSampling, the shape of y: {} \\n'.format(y_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==0)))\n",
    "print(\"After OverSampling, counts of label '2': {}\".format(sum(y_res==1)))\n",
    "\n",
    "\n",
    "X= numpy.array(X_res)\n",
    "y= numpy.array(y_res)\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "0.0    14612\n",
      "1.0    12362\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print( numpy.unique( predy ) )\n",
    "print(pd.value_counts(predy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-70159149e410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.add(Conv1D(32, kernel_size=(5, 5), strides=(1, 1),\n\u001b[1;32m      6\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                  input_shape=(1993)))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m                  \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                  **kwargs):\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tutorial-env/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 batch_input_shape = (\n\u001b[0;32m--> 147\u001b[0;31m                     batch_size,) + tuple(kwargs['input_shape'])\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "model = Sequential()\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(1993)))\n",
    "model.add(MaxPooling1D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv1D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64737 samples, validate on 16185 samples\n",
      "Epoch 1/20\n",
      "64737/64737 [==============================] - 2s 37us/step - loss: 0.6190 - acc: 0.6731 - val_loss: 0.5162 - val_acc: 0.8561\n",
      "Epoch 2/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.4390 - acc: 0.8035 - val_loss: 0.4511 - val_acc: 0.8833\n",
      "Epoch 3/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.3605 - acc: 0.8521 - val_loss: 0.3319 - val_acc: 0.9396\n",
      "Epoch 4/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.3077 - acc: 0.8821 - val_loss: 0.2929 - val_acc: 0.9444\n",
      "Epoch 5/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.2702 - acc: 0.8990 - val_loss: 0.2315 - val_acc: 0.9623\n",
      "Epoch 6/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.2473 - acc: 0.9111 - val_loss: 0.2057 - val_acc: 0.9661\n",
      "Epoch 7/20\n",
      "64737/64737 [==============================] - 2s 25us/step - loss: 0.2254 - acc: 0.9226 - val_loss: 0.1663 - val_acc: 0.9755\n",
      "Epoch 8/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.2079 - acc: 0.9284 - val_loss: 0.1558 - val_acc: 0.9769\n",
      "Epoch 9/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.1955 - acc: 0.9345 - val_loss: 0.1642 - val_acc: 0.9751\n",
      "Epoch 10/20\n",
      "64737/64737 [==============================] - 1s 23us/step - loss: 0.1807 - acc: 0.9409 - val_loss: 0.1432 - val_acc: 0.9778\n",
      "Epoch 11/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1707 - acc: 0.9459 - val_loss: 0.1305 - val_acc: 0.9825\n",
      "Epoch 12/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1646 - acc: 0.9493 - val_loss: 0.1132 - val_acc: 0.9889\n",
      "Epoch 13/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1607 - acc: 0.9521 - val_loss: 0.1057 - val_acc: 0.9876\n",
      "Epoch 14/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1530 - acc: 0.9549 - val_loss: 0.1121 - val_acc: 0.9871\n",
      "Epoch 15/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1483 - acc: 0.9565 - val_loss: 0.1049 - val_acc: 0.9885\n",
      "Epoch 16/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1469 - acc: 0.9567 - val_loss: 0.1099 - val_acc: 0.9891\n",
      "Epoch 17/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1380 - acc: 0.9594 - val_loss: 0.0899 - val_acc: 0.9922\n",
      "Epoch 18/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1390 - acc: 0.9599 - val_loss: 0.0979 - val_acc: 0.9917\n",
      "Epoch 19/20\n",
      "64737/64737 [==============================] - 2s 23us/step - loss: 0.1317 - acc: 0.9617 - val_loss: 0.0942 - val_acc: 0.9922\n",
      "Epoch 20/20\n",
      "64737/64737 [==============================] - 2s 24us/step - loss: 0.1316 - acc: 0.9634 - val_loss: 0.0933 - val_acc: 0.9912\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, random_state=999)\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1993, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=208,\n",
    "          validation_split=0.2)\n",
    "\n",
    "#score = model.evaluate(X_val, y_val, batch_size=128)\n",
    "#print(model.metrics_names)\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "written_test =numpy.load(\"written_test(1).npy\")\n",
    "spoken_test =numpy.load(\"spoken_test(1).npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 93, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., -1,  0,  1],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ..., -2,  0,  1],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., -2, -1,  0],\n",
       "       [ 0,  0,  0, ..., -2, -1,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "maxs = []\n",
    "for i in range(0, len(spoken_test)):\n",
    "    maxs.append(spoken_test[i].shape[0])\n",
    "maxlen_spoken_test = max(maxs)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "spoken_test_3d= pad_sequences(spoken_test, maxlen= max(maxs))\n",
    "print(spoken_test_3d.shape)\n",
    "\n",
    "spoken_test_2d =spoken_test_3d.reshape(15000, maxlen_spoken_test*feature_amount)\n",
    "\n",
    "spoken_test_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "spoken_test_mmsc = mm_scaler.fit_transform(spoken_test_2d)\n",
    "written_test_mmsc = mm_scaler.fit_transform(written_test)\n",
    "Scaler = \"MinMax\"\n",
    "\n",
    "#When using MinMax\n",
    "X_test = numpy.hstack([spoken_test_mmsc, written_test_mmsc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS_scaler = StandardScaler()\n",
    "spoken_test_SSscaler = SS_scaler.fit_transform(spoken_test_2d)\n",
    "written_test_SSscaler = SS_scaler.fit_transform(written_test)\n",
    "\n",
    "Scaler = \"Standard scaler\"\n",
    "\n",
    "#When using standard scaler\n",
    "X_test = numpy.hstack([spoken_test_SSscaler, written_test_SSscaler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Abs\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "MaxAbs_scaler = MaxAbsScaler()\n",
    "spoken_test_MaxAbsSc = MaxAbs_scaler.fit_transform(spoken_test_2d)\n",
    "written_test_MaxAbsSc = MaxAbs_scaler.fit_transform(written_test)\n",
    "Scaler = \"Max Abs\"\n",
    "\n",
    "#When using Max Abs\n",
    "X_test = numpy.hstack([spoken_test_MaxAbsSc, written_test_MaxAbsSc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "spoken_test_normalized = normalize(spoken_test_2d, norm='l2')\n",
    "written_test_normalized = normalize(written_test, norm='l2')\n",
    "Scaler = \"Normalized\"\n",
    "\n",
    "#When using Normalized\n",
    "X_test = numpy.hstack([spoken_test_normalized, written_test_normalized])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1993)\n"
     ]
    }
   ],
   "source": [
    "#When using NOTHING!!!\n",
    "\n",
    "X_test = numpy.hstack([spoken_test_2d, written_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 1s 47us/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "True\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict_classes(X_test, verbose=1)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(1 in y_pred)\n",
    "y_pred = y_pred.reshape(15000,)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save(\"attempttest9939.npy\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
